{
  data_module: {
    type: map

    setup: {
      dataset_files: {
        train_data_path: "data/train_trx.parquet"
        test_data_path: "data/test_trx.parquet"
      }
      col_id: cl_id
      col_id_dtype: int
      col_target: target_flag

      split_by: embeddings_validation
      fold_info: "conf/embeddings_validation.work/folds/folds.json"
    }

    train: {
        min_seq_len: 0
        augmentations: [
            [RandomSlice, {min_len: 250, max_len: 350, rate_for_min: 0.9}]
            [DropoutTrx, {trx_dropout: 0.01}]
            [SeqLenLimit, {max_seq_len: 1200}]
        ]
        num_workers: 16
        batch_size: 32
    }

    valid: {
        augmentations: [
            [SeqLenLimit, {max_seq_len: 1200}]
        ]
        num_workers: 8
        batch_size: 512
    }
  }

  embedding_validation_results: {
    model_name: nn
    feature_name: nsp_finetuning
    output_path: "results/nsp_finetuning_results.json"
  }

  seed_everything: 42
  trainer: {
    gpus: 1
    auto_select_gpus: false

    max_epochs: 4
    log_every_n_steps: 10

    checkpoint_callback: false
    deterministic: true
  }
  logger_name: nsp_finetuning

  params: {
    model_type: rnn,
    rnn: {
      hidden_size: 512
    },
    ensemble_size: 1,
    trx_encoder: {
      embeddings: {
        mcc: {in: 100, out: 24},
        channel_type: {in: 4, out: 4},
        currency: {in: 4, out: 4},
        trx_category: {in: 10, out: 2}
      },
      numeric_values: {
        amount: identity
      }
    },

    score_metric: auroc,
    norm_scores: false,
    use_batch_norm: true,

    encoder_type: pretrained,
    pretrained: {
        pl_module_class: dltranz.lightning_modules.sop_nsp_module.SopNspModule
        model_path: "models/nsp_model.p"
        lr: 0.0001
    }

    head_layers: [
        [BatchNorm1d, {num_features: "{seq_encoder.embedding_size}"}]
        [Linear, {"in_features": "{seq_encoder.embedding_size}", "out_features": 1}]
        [Sigmoid, {}]
        [Squeeze, {}]
    ]

    lr_scheduler: {
     CosineAnnealing: true,
    },

    train: {
      random_neg: false,
      loss: bce,
      lr: 0.0001,
      weight_decay: 0.0,
      n_epoch: 10
      optimiser_params: {
        0.: {lr: 0.0001},
        1.: {lr: 0.0001}
      }
      lr_scheduler: {
        n_epoch: 10,
      },
    }
  }
}
