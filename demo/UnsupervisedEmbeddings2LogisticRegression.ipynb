{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ddef53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7431993",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a798aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../../data’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239M  100  239M    0     0  49.4M      0  0:00:04  0:00:04 --:--:-- 54.9M02k      0  0:05:49 --:--:--  0:05:49  701k\n",
      "Archive:  age-prediction-nti-sbebank-2019.zip\n",
      "  inflating: ../../data/test.csv     \n",
      "  inflating: ../../data/small_group_description.csv  \n",
      "  inflating: ../../data/train_target.csv  \n",
      "  inflating: ../../data/transactions_train.csv  \n",
      "  inflating: ../../data/transactions_test.csv  \n"
     ]
    }
   ],
   "source": [
    "! mkdir ../../data\n",
    "! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d ../../data\n",
    "! mv age-prediction-nti-sbebank-2019.zip ../../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"PysparkDataPreprocessor\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "59d771ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3a9e0909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "|    33172|         8|         11|    13.887|\n",
      "|    33172|         9|         11|    15.983|\n",
      "|    33172|        10|         11|    21.341|\n",
      "|    33172|        11|         11|    17.941|\n",
      "|    33172|        12|         11|    17.726|\n",
      "|    33172|        13|         18|    47.397|\n",
      "|    33172|        13|          1|   220.009|\n",
      "|    33172|        13|         11|     9.067|\n",
      "|    33172|        16|          3|    18.319|\n",
      "|    33172|        16|          1|     9.846|\n",
      "|    33172|        16|         11|    19.666|\n",
      "|    33172|        17|         82|     2.544|\n",
      "|    33172|        19|          3|    16.388|\n",
      "|    33172|        19|         32|    45.795|\n",
      "|    33172|        19|          1|     4.184|\n",
      "|    33172|        19|         11|    15.479|\n",
      "|    33172|        20|          1|     35.17|\n",
      "|    33172|        23|         11|    18.847|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = source_data.alias('df')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from .base import DataPreprocessor\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PysparkDataPreprocessor(DataPreprocessor):\n",
    "    \"\"\"Data preprocessor based on pandas.DataFrame\n",
    "    During preprocessing it\n",
    "        * transform `cols_event_time` column with date and time\n",
    "        * encodes category columns `cols_category` into ints;\n",
    "        * apply logarithm transformation to `cols_log_norm' columns;\n",
    "        * groups flat data by `col_id`;\n",
    "        * arranges data into list of dicts with features\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_id : str\n",
    "        name of column with ids\n",
    "    cols_event_time : str,\n",
    "        name of column with time and date\n",
    "    cols_category : list[str],s\n",
    "        list of category columns\n",
    "    cols_log_norm : list[str],\n",
    "        list of columns to be logarithmed\n",
    "    cols_identity : list[str],\n",
    "        list of columns to be passed as is without any transformation\n",
    "    cols_target: List[str],\n",
    "        list of columns with target\n",
    "    time_transformation: str. Default: 'default'.\n",
    "        type of transformation to be applied to time column\n",
    "    print_dataset_info : bool. Default: False.\n",
    "        If True, print dataset stats during preprocessor fitting and data transformation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 col_id: str,\n",
    "                 cols_event_time: str,\n",
    "                 cols_category: List[str],\n",
    "                 cols_log_norm: List[str],\n",
    "                 cols_identity: List[str],\n",
    "                 cols_target: List[str] = [],\n",
    "                 time_transformation: str = 'default'):\n",
    "\n",
    "        super().__init__(col_id, cols_event_time, cols_category, cols_log_norm, cols_identity, cols_target)\n",
    "        self.print_dataset_info = print_dataset_info\n",
    "        self.time_transformation = time_transformation\n",
    "        self.time_min = None\n",
    "        \n",
    "        \n",
    "    def fit(self, df, **params):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dt : pandas.DataFrame with flat data\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted preprocessor.\n",
    "        \"\"\"\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            mapping = {row[col]: i + 1 for i, row in enumerate(df.select(col).distinct().collect())}\n",
    "            self.cols_category_mapping[col] = mapping\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df = df.withColumn('sign', F.when(F.col(col) >= 0, 1).otherwise(-1))\n",
    "            self.cols_log_norm_maxes[col] = source_data.select((F.log1p(F.abs(F.col(col))) * F.col('sign')).alias('log1p_signed'))\\\n",
    "                                                                    .agg({\"log1p_signed\": \"max\"}).collect()[0]['max(log1p_signed)']\n",
    "\n",
    "        if self.time_transformation == 'hours_from_min':\n",
    "            self.time_min = df.select((F.col(self.cols_event_time))\\\n",
    "                                      .cast(dataType=T.TimestampType()).alias('dt'))\\\n",
    "                                      .agg({'dt': 'min'}).collect()[0]['min(dt)']\n",
    "            self.time_min = (self.time_min - datetime.datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, df, copy=True):\n",
    "        \"\"\"Perform preprocessing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame with flat data\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "        Returns\n",
    "        -------\n",
    "        features : List of dicts grouped by col_id.\n",
    "        \"\"\"\n",
    "        self.check_is_fitted()\n",
    "        df_data = df.alias('df_data') if copy else df\n",
    "\n",
    "        # event_time mapping\n",
    "        if self.time_transformation == 'none':\n",
    "            pass\n",
    "        elif self.time_transformation == 'default':\n",
    "            df_data = self._td_default(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'float':\n",
    "            df_data = self._td_float(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'gender':\n",
    "            df_data = self._td_gender(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'hours_from_min':\n",
    "            df_data = self._td_hours(df_data, self.cols_event_time)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown type of data transformation: \"{self.time_transformation}\"')\n",
    "        \n",
    "\n",
    "    def TRANSFORM(self, df, copy=True):\n",
    "        \"\"\"Perform preprocessing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame with flat data\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "        Returns\n",
    "        -------\n",
    "        features : List of dicts grouped by col_id.\n",
    "        \"\"\"\n",
    "        self.check_is_fitted()\n",
    "        df_data = df.copy() if copy else df\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            logger.info(f'Found {df_data[self.col_id].nunique()} unique ids')\n",
    "\n",
    "        # event_time mapping\n",
    "        if self.time_transformation == 'none':\n",
    "            pass\n",
    "        elif self.time_transformation == 'default':\n",
    "            df_data = self._td_default(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'float':\n",
    "            df_data = self._td_float(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'gender':\n",
    "            df_data = self._td_gender(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'hours_from_min':\n",
    "            df_data = self._td_hours(df_data, self.cols_event_time)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown type of data transformation: \"{self.time_transformation}\"')\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            if col not in self.cols_category_mapping:\n",
    "                raise KeyError(f\"column {col} isn't in fitted category columns\")\n",
    "            pd_col = df_data[col].astype(str)\n",
    "            df_data[col] = pd_col.map(self.cols_category_mapping[col]) \\\n",
    "                .fillna(max(self.cols_category_mapping[col].values()))\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(df_data[col], col)}')\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df_data[col] = np.log1p(abs(df_data[col])) * np.sign(df_data[col])\n",
    "            df_data[col] /= self.cols_log_norm_maxes[col]\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(df_data[col], col)}')\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            df = df_data.groupby(self.col_id)['event_time'].count()\n",
    "            logger.info(f'Trx count per clients:\\nlen(trx_list) | client_count\\n{pd_hist(df, \"trx_count\")}')\n",
    "\n",
    "        # column filter\n",
    "        columns_for_filter = reduce(iadd, [\n",
    "            self.cols_category,\n",
    "            self.cols_log_norm,\n",
    "            self.cols_identity,\n",
    "            ['event_time', self.col_id],\n",
    "            self.cols_target,\n",
    "        ], [])\n",
    "        used_columns = [col for col in df_data.columns if col in columns_for_filter]\n",
    "\n",
    "        logger.info('Feature collection in progress ...')\n",
    "        features = df_data[used_columns] \\\n",
    "            .assign(et_index=lambda x: x['event_time']) \\\n",
    "            .set_index([self.col_id, 'et_index']).sort_index() \\\n",
    "            .groupby(self.col_id).apply(lambda x: {k: v[0] if k in self.cols_target else np.array(v)\n",
    "                                                   for k, v in x.to_dict(orient='list').items()}) \\\n",
    "            .rename('feature_arrays').reset_index().to_dict(orient='records')\n",
    "\n",
    "        def squeeze(rec):\n",
    "            return {self.col_id: rec[self.col_id], **rec['feature_arrays']}\n",
    "        features = [squeeze(r) for r in features]\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            feature_names = list(features[0].keys())\n",
    "            logger.info(f'Feature names: {feature_names}')\n",
    "\n",
    "        logger.info(f'Prepared features for {len(features)} clients')\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _td_default(df, cols_event_time):\n",
    "        w = Window().orderBy(cols_event_time)\n",
    "        tmp_df = df.select(cols_event_time).distinct()\n",
    "        tmp_df = tmp_df.withColumn('event_time', F.row_number().over(w) - 1)\n",
    "        df = df.join(tmp_df, on=cols_event_time)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _td_float(df, col_event_time):\n",
    "        logger.info('To-float time transformation begins...')\n",
    "        df = df.withColumn('event_time', F.col(col_event_time).astype('float'))\n",
    "        logger.info('To-float time transformation ends')\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _td_gender(df, col_event_time):\n",
    "        \"\"\"Gender-dataset-like transformation\n",
    "        'd hh:mm:ss' -> float where integer part is day number and fractional part is seconds from day begin\n",
    "        '1 00:00:00' -> 1.0\n",
    "        '1 12:00:00' -> 1.5\n",
    "        '1 01:00:00' -> 1 + 1 / 24\n",
    "        '2 23:59:59' -> 1.99\n",
    "        '432 12:00:00' -> 432.5   '000432 12:00:00'\n",
    "        :param df:\n",
    "        :param col_event_time:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logger.info('Gender-dataset-like time transformation begins...')\n",
    "        df = df.withColumn('_et_day', F.substring(F.lpad(F.col(col_event_time), 15, '0'), 1, 6).cast('float'))\n",
    "\n",
    "        df = df.withColumn('_et_time', F.substring(F.lpad(F.col(col_event_time), 15, '0'), 8, 8))\n",
    "        df = df.withColumn('_et_time', F.regexp_replace('_et_time', r'\\:60$', ':59'))\n",
    "        df = df.withColumn('_et_time', F.unix_timestamp('_et_time', 'HH:mm:ss') / (24 * 60 * 60))\n",
    "\n",
    "        df = df.withColumn('event_time', F.col('_et_day') + F.col('_et_time'))\n",
    "        df = df.drop('_et_day', '_et_time')\n",
    "        logger.info('Gender-dataset-like time transformation ends')\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def _td_hours(self, df, col_event_time):\n",
    "        logger.info('To hours time transformation begins...')\n",
    "        df = df.withColumn('_dt', (F.col(col_event_time)).cast(dataType=T.TimestampType()))\n",
    "        df = df.withColumn('event_time', ((F.col('_dt')).cast('float') - self.time_min) / 3600)\n",
    "        df = df.drop('_dt')\n",
    "        logger.info('To hours time transformation ends')\n",
    "        return df\n",
    "\n",
    "   \n",
    "    def _reset(self):\n",
    "        \"\"\"Reset internal data-dependent state of the preprocessor, if necessary.\n",
    "        __init__ parameters are not touched.\n",
    "        \"\"\"\n",
    "        self.time_min = None\n",
    "        super()._reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7e06bd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   client_id  trans_date  small_group  amount_rur\n",
       "0      33172           6            4      71.463\n",
       "1      33172           6           35      45.017"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "source_data_pd = pd.read_csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3587fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = source_data_pd.copy()\n",
    "cols_event_time = 'trans_date'\n",
    "\n",
    "df_event_time = pd.DataFrame({cols_event_time: df[cols_event_time].drop_duplicates()})\n",
    "df_event_time = df_event_time.sort_values(cols_event_time)\n",
    "df_event_time['event_time'] = np.arange(len(df_event_time))\n",
    "df = pd.merge(df, df_event_time, on=cols_event_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8c3056ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44477</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>17.879</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44477</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>122.482</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1049</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>19.418</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450572</th>\n",
       "      <td>22890</td>\n",
       "      <td>22</td>\n",
       "      <td>166</td>\n",
       "      <td>29.774</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450573</th>\n",
       "      <td>22890</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>23.763</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450574</th>\n",
       "      <td>9281</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>12.636</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450575</th>\n",
       "      <td>49473</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0.938</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450576</th>\n",
       "      <td>36371</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>22.632</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26450577 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          client_id  trans_date  small_group  amount_rur  event_time\n",
       "0             33172           6            4      71.463           6\n",
       "1             33172           6           35      45.017           6\n",
       "2             44477           6            4      17.879           6\n",
       "3             44477           6            1     122.482           6\n",
       "4              1049           6           36      19.418           6\n",
       "...             ...         ...          ...         ...         ...\n",
       "26450572      22890          22          166      29.774          22\n",
       "26450573      22890          22            1      23.763          22\n",
       "26450574       9281          22            1      12.636          22\n",
       "26450575      49473          22           25       0.938          22\n",
       "26450576      36371          22            1      22.632          22\n",
       "\n",
       "[26450577 rows x 5 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5f846a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44477</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>17.879</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44477</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>122.482</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1049</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>19.418</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450572</th>\n",
       "      <td>22890</td>\n",
       "      <td>22</td>\n",
       "      <td>166</td>\n",
       "      <td>29.774</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450573</th>\n",
       "      <td>22890</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>23.763</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450574</th>\n",
       "      <td>9281</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>12.636</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450575</th>\n",
       "      <td>49473</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0.938</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450576</th>\n",
       "      <td>36371</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>22.632</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26450577 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          client_id  trans_date  small_group  amount_rur  event_time\n",
       "0             33172           6            4      71.463           6\n",
       "1             33172           6           35      45.017           6\n",
       "2             44477           6            4      17.879           6\n",
       "3             44477           6            1     122.482           6\n",
       "4              1049           6           36      19.418           6\n",
       "...             ...         ...          ...         ...         ...\n",
       "26450572      22890          22          166      29.774          22\n",
       "26450573      22890          22            1      23.763          22\n",
       "26450574       9281          22            1      12.636          22\n",
       "26450575      49473          22           25       0.938          22\n",
       "26450576      36371          22            1      22.632          22\n",
       "\n",
       "[26450577 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75830aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6152d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.to_datetime(source_data_pd['trans_date']).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df1737c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = source_data_pd.copy()\n",
    "\n",
    "len(df_copy['trans_date'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1602c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26450577"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c67b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58044b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1970-01-01 00:00:00.000000006\n",
       "1          1970-01-01 00:00:00.000000006\n",
       "2          1970-01-01 00:00:00.000000008\n",
       "3          1970-01-01 00:00:00.000000009\n",
       "4          1970-01-01 00:00:00.000000010\n",
       "                        ...             \n",
       "26450572   1970-01-01 00:00:00.000000727\n",
       "26450573   1970-01-01 00:00:00.000000727\n",
       "26450574   1970-01-01 00:00:00.000000727\n",
       "26450575   1970-01-01 00:00:00.000000727\n",
       "26450576   1970-01-01 00:00:00.000000729\n",
       "Name: trans_date, Length: 26450577, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(source_data_pd['trans_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd5538b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.899439277009005"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = source_data_pd\n",
    "\n",
    "(np.log1p(abs(dt[col])) * np.sign(dt[col])).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8615b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_preprocessing import PandasDataPreprocessor\n",
    "\n",
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    cols_event_time='trans_date',\n",
    "    time_transformation='float',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[\"amount_rur\"],\n",
    "    cols_identity=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca72f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 3.2 s, total: 53.6 s\n",
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = preprocessor.fit_transform(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e7d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e2418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9080a6f",
   "metadata": {},
   "source": [
    "## Embedding training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56493c0b",
   "metadata": {},
   "source": [
    "Model training in our framework organised via pytorch-lightning (pl) framework.\n",
    "The key parts of neural networks training in pl are: \n",
    "\n",
    "    * model (pl.LightningModule)\n",
    "    * data_module (pl.LightningDataModule)\n",
    "    * pl.trainer (pl.trainer)\n",
    "    \n",
    "For futher details check https://www.pytorchlightning.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6ee58",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988c508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder import SequenceEncoder\n",
    "from dltranz.models import Head\n",
    "from dltranz.lightning_modules.emb_module import EmbModule\n",
    "\n",
    "seq_encoder = SequenceEncoder(\n",
    "    category_features=preprocessor.get_category_sizes(),\n",
    "    numeric_features=[\"amount_rur\"],\n",
    "    trx_embedding_noize=0.003\n",
    ")\n",
    "\n",
    "head = Head(input_size=seq_encoder.embedding_size, use_norm_encoder=True)\n",
    "\n",
    "model = EmbModule(seq_encoder=seq_encoder, head=head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87339c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87997ac0",
   "metadata": {},
   "source": [
    "### Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624065bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule\n",
    "\n",
    "dm = EmbeddingTrainDataModule(\n",
    "    dataset=train,\n",
    "    pl_module=model,\n",
    "    min_seq_len=25,\n",
    "    seq_split_strategy='SampleSlices',\n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    "    split_count=5,\n",
    "    split_cnt_min=25,\n",
    "    split_cnt_max=200,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a09be",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdbb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "# logging.getLogger(\"lightning\").addHandler(logging.NullHandler())\n",
    "# logging.getLogger(\"lightning\").propagate = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "#     progress_bar_refresh_rate=0,\n",
    "    max_epochs=150,\n",
    "    gpus=1 if torch.cuda.is_available() else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88078a3",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40877df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c32741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24000it [00:01, 21719.33it/s]\n",
      "6000it [00:00, 31278.41it/s]                                                                                                            \n",
      "                                                                                                                                        \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((24000, 512), (6000, 512))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding inference\n",
    "\n",
    "from dltranz.inference import get_embeddings\n",
    "\n",
    "train_embeds = get_embeddings(\n",
    "    data=train,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "test_embeds = get_embeddings(\n",
    "    data=test,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18245f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 514) (6000, 514)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_504</th>\n",
       "      <th>embed_505</th>\n",
       "      <th>embed_506</th>\n",
       "      <th>embed_507</th>\n",
       "      <th>embed_508</th>\n",
       "      <th>embed_509</th>\n",
       "      <th>embed_510</th>\n",
       "      <th>embed_511</th>\n",
       "      <th>client_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344823</td>\n",
       "      <td>0.340152</td>\n",
       "      <td>0.231864</td>\n",
       "      <td>-0.789117</td>\n",
       "      <td>-0.013289</td>\n",
       "      <td>-0.056129</td>\n",
       "      <td>-0.988241</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>-0.064898</td>\n",
       "      <td>-0.029179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342959</td>\n",
       "      <td>0.040367</td>\n",
       "      <td>0.253053</td>\n",
       "      <td>0.712581</td>\n",
       "      <td>-0.148498</td>\n",
       "      <td>0.016645</td>\n",
       "      <td>-0.124844</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>36253</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276711</td>\n",
       "      <td>0.492666</td>\n",
       "      <td>0.781279</td>\n",
       "      <td>-0.824952</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>-0.014695</td>\n",
       "      <td>-0.891105</td>\n",
       "      <td>-0.047485</td>\n",
       "      <td>0.063514</td>\n",
       "      <td>0.170822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372329</td>\n",
       "      <td>0.049816</td>\n",
       "      <td>0.346733</td>\n",
       "      <td>0.465750</td>\n",
       "      <td>-0.080580</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>-0.040877</td>\n",
       "      <td>-0.028394</td>\n",
       "      <td>396</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0  0.344823  0.340152  0.231864 -0.789117 -0.013289 -0.056129 -0.988241   \n",
       "1  0.276711  0.492666  0.781279 -0.824952  0.020340 -0.014695 -0.891105   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_504  embed_505  embed_506  \\\n",
       "0 -0.010464 -0.064898 -0.029179  ...   0.342959   0.040367   0.253053   \n",
       "1 -0.047485  0.063514  0.170822  ...   0.372329   0.049816   0.346733   \n",
       "\n",
       "   embed_507  embed_508  embed_509  embed_510  embed_511  client_id  target  \n",
       "0   0.712581  -0.148498   0.016645  -0.124844  -0.078120      36253       1  \n",
       "1   0.465750  -0.080580   0.011563  -0.040877  -0.028394        396       2  \n",
       "\n",
       "[2 rows x 514 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join target and embeddings\n",
    "\n",
    "df_target = pd.read_csv(os.path.join(data_path, 'train_target.csv'))\n",
    "df_target = df_target.set_index('client_id')\n",
    "df_target.rename(columns={\"bins\": \"target\"}, inplace=True)\n",
    "\n",
    "train_df = pd.DataFrame(data=train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n",
    "train_df['client_id'] = [x['client_id'] for x in train]\n",
    "train_df = train_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "test_df = pd.DataFrame(data=test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n",
    "test_df['client_id'] = [x['client_id'] for x in test]\n",
    "test_df = test_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa0c0",
   "metadata": {},
   "source": [
    "Obtained embeddings can be used as features for model training\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e3de46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embed_columns = [x for x in train_df.columns if x.startswith('embed')]\n",
    "x_train, y_train = train_df[embed_columns], train_df['target']\n",
    "x_test, y_test = test_df[embed_columns], test_df['target']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
