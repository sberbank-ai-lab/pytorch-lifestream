{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7431993",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ../../data\n",
    "! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d ../../data\n",
    "! mv age-prediction-nti-sbebank-2019.zip ../../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mikheev/.local/share/virtualenvs/pytorch-lifestream-sv0mOraz/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/11 18:17:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"PysparkDataPreprocessor\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04de38be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = spark.read.option(\"header\",True).csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd359e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0660b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  dt|\n",
      "+----+\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pd.to_datetime(dt[self.cols_event_time]).min()\n",
    "\n",
    "source_data.select(F.to_timestamp(source_data.trans_date).alias('dt')).show()   #.agg({'dt': 'min'}).collect()[0]['min(dt)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from .base import DataPreprocessor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PysparkDataPreprocessor(DataPreprocessor):\n",
    "    \"\"\"Data preprocessor based on pandas.DataFrame\n",
    "    During preprocessing it\n",
    "        * transform `cols_event_time` column with date and time\n",
    "        * encodes category columns `cols_category` into ints;\n",
    "        * apply logarithm transformation to `cols_log_norm' columns;\n",
    "        * groups flat data by `col_id`;\n",
    "        * arranges data into list of dicts with features\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_id : str\n",
    "        name of column with ids\n",
    "    cols_event_time : str,\n",
    "        name of column with time and date\n",
    "    cols_category : list[str],s\n",
    "        list of category columns\n",
    "    cols_log_norm : list[str],\n",
    "        list of columns to be logarithmed\n",
    "    cols_identity : list[str],\n",
    "        list of columns to be passed as is without any transformation\n",
    "    cols_target: List[str],\n",
    "        list of columns with target\n",
    "    time_transformation: str. Default: 'default'.\n",
    "        type of transformation to be applied to time column\n",
    "    print_dataset_info : bool. Default: False.\n",
    "        If True, print dataset stats during preprocessor fitting and data transformation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 col_id: str,\n",
    "                 cols_event_time: str,\n",
    "                 cols_category: List[str],\n",
    "                 cols_log_norm: List[str],\n",
    "                 cols_identity: List[str],\n",
    "                 cols_target: List[str] = [],\n",
    "                 time_transformation: str = 'default'):\n",
    "\n",
    "        super().__init__(col_id, cols_event_time, cols_category, cols_log_norm, cols_identity, cols_target)\n",
    "        self.print_dataset_info = print_dataset_info\n",
    "        self.time_transformation = time_transformation\n",
    "        self.time_min = None\n",
    "        \n",
    "        \n",
    "    def fit(self, dt, **params):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dt : pandas.DataFrame with flat data\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted preprocessor.\n",
    "        \"\"\"\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            mapping = {row[col]: i + 1 for i, row in enumerate(dt.select(col).distinct().collect())}\n",
    "            self.cols_category_mapping[col] = mapping\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            dt = dt.withColumn('sign', F.when(F.col(col) >= 0, 1).otherwise(-1))\n",
    "            self.cols_log_norm_maxes[col] = source_data.select((F.log1p(F.abs(F.col(col))) * F.col('sign')).alias('log1p_signed'))\\\n",
    "                                                                    .agg({\"log1p_signed\": \"max\"}).collect()[0]['max(log1p_signed)']\n",
    "\n",
    "        if self.time_transformation == 'hours_from_min':\n",
    "            self.time_min = pd.to_datetime(dt[self.cols_event_time]).min()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset internal data-dependent state of the preprocessor, if necessary.\n",
    "        __init__ parameters are not touched.\n",
    "        \"\"\"\n",
    "        # TODO: pyspark reset data-dependent state of the preprocessor\n",
    "        pass\n",
    "\n",
    "    def fit(self, dt, **params):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dt : pandas.DataFrame with flat data\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted preprocessor.\n",
    "        \"\"\"\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "\n",
    "        # TODO: pyspark fit\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, dt, **params):\n",
    "        self.check_is_fitted()\n",
    "\n",
    "        # TODO: pyspark transformation\n",
    "\n",
    "        return dt\n",
    "\n",
    "    def check_is_fitted(self):\n",
    "        # TODO: pyspark is preprocessor fitted check\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e06bd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   client_id  trans_date  small_group  amount_rur\n",
       "0      33172           6            4      71.463\n",
       "1      33172           6           35      45.017"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "source_data_pd = pd.read_csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b2c52b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1970-01-01 00:00:00.000000006\n",
       "1          1970-01-01 00:00:00.000000006\n",
       "2          1970-01-01 00:00:00.000000008\n",
       "3          1970-01-01 00:00:00.000000009\n",
       "4          1970-01-01 00:00:00.000000010\n",
       "                        ...             \n",
       "26450572   1970-01-01 00:00:00.000000727\n",
       "26450573   1970-01-01 00:00:00.000000727\n",
       "26450574   1970-01-01 00:00:00.000000727\n",
       "26450575   1970-01-01 00:00:00.000000727\n",
       "26450576   1970-01-01 00:00:00.000000729\n",
       "Name: trans_date, Length: 26450577, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(source_data_pd['trans_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1d732f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.899439277009005"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = source_data_pd\n",
    "\n",
    "(np.log1p(abs(dt[col])) * np.sign(dt[col])).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8615b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_preprocessing import PandasDataPreprocessor\n",
    "\n",
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    cols_event_time='trans_date',\n",
    "    time_transformation='float',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[\"amount_rur\"],\n",
    "    cols_identity=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca72f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 3.2 s, total: 53.6 s\n",
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = preprocessor.fit_transform(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e7d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e2418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9080a6f",
   "metadata": {},
   "source": [
    "## Embedding training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56493c0b",
   "metadata": {},
   "source": [
    "Model training in our framework organised via pytorch-lightning (pl) framework.\n",
    "The key parts of neural networks training in pl are: \n",
    "\n",
    "    * model (pl.LightningModule)\n",
    "    * data_module (pl.LightningDataModule)\n",
    "    * pl.trainer (pl.trainer)\n",
    "    \n",
    "For futher details check https://www.pytorchlightning.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6ee58",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988c508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder import SequenceEncoder\n",
    "from dltranz.models import Head\n",
    "from dltranz.lightning_modules.emb_module import EmbModule\n",
    "\n",
    "seq_encoder = SequenceEncoder(\n",
    "    category_features=preprocessor.get_category_sizes(),\n",
    "    numeric_features=[\"amount_rur\"],\n",
    "    trx_embedding_noize=0.003\n",
    ")\n",
    "\n",
    "head = Head(input_size=seq_encoder.embedding_size, use_norm_encoder=True)\n",
    "\n",
    "model = EmbModule(seq_encoder=seq_encoder, head=head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87339c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87997ac0",
   "metadata": {},
   "source": [
    "### Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624065bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule\n",
    "\n",
    "dm = EmbeddingTrainDataModule(\n",
    "    dataset=train,\n",
    "    pl_module=model,\n",
    "    min_seq_len=25,\n",
    "    seq_split_strategy='SampleSlices',\n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    "    split_count=5,\n",
    "    split_cnt_min=25,\n",
    "    split_cnt_max=200,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a09be",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdbb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "# logging.getLogger(\"lightning\").addHandler(logging.NullHandler())\n",
    "# logging.getLogger(\"lightning\").propagate = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "#     progress_bar_refresh_rate=0,\n",
    "    max_epochs=150,\n",
    "    gpus=1 if torch.cuda.is_available() else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88078a3",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40877df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c32741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24000it [00:01, 21719.33it/s]\n",
      "6000it [00:00, 31278.41it/s]                                                                                                            \n",
      "                                                                                                                                        \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((24000, 512), (6000, 512))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding inference\n",
    "\n",
    "from dltranz.inference import get_embeddings\n",
    "\n",
    "train_embeds = get_embeddings(\n",
    "    data=train,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "test_embeds = get_embeddings(\n",
    "    data=test,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18245f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 514) (6000, 514)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_504</th>\n",
       "      <th>embed_505</th>\n",
       "      <th>embed_506</th>\n",
       "      <th>embed_507</th>\n",
       "      <th>embed_508</th>\n",
       "      <th>embed_509</th>\n",
       "      <th>embed_510</th>\n",
       "      <th>embed_511</th>\n",
       "      <th>client_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344823</td>\n",
       "      <td>0.340152</td>\n",
       "      <td>0.231864</td>\n",
       "      <td>-0.789117</td>\n",
       "      <td>-0.013289</td>\n",
       "      <td>-0.056129</td>\n",
       "      <td>-0.988241</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>-0.064898</td>\n",
       "      <td>-0.029179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342959</td>\n",
       "      <td>0.040367</td>\n",
       "      <td>0.253053</td>\n",
       "      <td>0.712581</td>\n",
       "      <td>-0.148498</td>\n",
       "      <td>0.016645</td>\n",
       "      <td>-0.124844</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>36253</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276711</td>\n",
       "      <td>0.492666</td>\n",
       "      <td>0.781279</td>\n",
       "      <td>-0.824952</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>-0.014695</td>\n",
       "      <td>-0.891105</td>\n",
       "      <td>-0.047485</td>\n",
       "      <td>0.063514</td>\n",
       "      <td>0.170822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372329</td>\n",
       "      <td>0.049816</td>\n",
       "      <td>0.346733</td>\n",
       "      <td>0.465750</td>\n",
       "      <td>-0.080580</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>-0.040877</td>\n",
       "      <td>-0.028394</td>\n",
       "      <td>396</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0  0.344823  0.340152  0.231864 -0.789117 -0.013289 -0.056129 -0.988241   \n",
       "1  0.276711  0.492666  0.781279 -0.824952  0.020340 -0.014695 -0.891105   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_504  embed_505  embed_506  \\\n",
       "0 -0.010464 -0.064898 -0.029179  ...   0.342959   0.040367   0.253053   \n",
       "1 -0.047485  0.063514  0.170822  ...   0.372329   0.049816   0.346733   \n",
       "\n",
       "   embed_507  embed_508  embed_509  embed_510  embed_511  client_id  target  \n",
       "0   0.712581  -0.148498   0.016645  -0.124844  -0.078120      36253       1  \n",
       "1   0.465750  -0.080580   0.011563  -0.040877  -0.028394        396       2  \n",
       "\n",
       "[2 rows x 514 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join target and embeddings\n",
    "\n",
    "df_target = pd.read_csv(os.path.join(data_path, 'train_target.csv'))\n",
    "df_target = df_target.set_index('client_id')\n",
    "df_target.rename(columns={\"bins\": \"target\"}, inplace=True)\n",
    "\n",
    "train_df = pd.DataFrame(data=train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n",
    "train_df['client_id'] = [x['client_id'] for x in train]\n",
    "train_df = train_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "test_df = pd.DataFrame(data=test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n",
    "test_df['client_id'] = [x['client_id'] for x in test]\n",
    "test_df = test_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa0c0",
   "metadata": {},
   "source": [
    "Obtained embeddings can be used as features for model training\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e3de46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embed_columns = [x for x in train_df.columns if x.startswith('embed')]\n",
    "x_train, y_train = train_df[embed_columns], train_df['target']\n",
    "x_test, y_test = test_df[embed_columns], test_df['target']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
