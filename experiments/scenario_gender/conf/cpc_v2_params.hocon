{
  data_module: {
    type: map
    setup: {
      col_id: game_session
      dataset_files: {
        data_path: "data/train_trx.parquet"
      }
      split_by: files
      valid_size: 0.05
      valid_split_seed: 42
    }
    
    train: {
      min_seq_len: 200  # split_count * 5
      augmentations: [
        [SeqLenLimit, {max_seq_len: 1200}]
      ]
      split_strategy: {
        split_strategy: "SampleUniformBySplitCount"
        split_count: 40
      }
      num_workers: 16
      batch_size: 128
    }

    valid: {
      augmentations: [
        [SeqLenLimit, {max_seq_len: 1200}]
      ]
      split_strategy: {
        split_strategy: "SampleUniformBySplitCount"
        split_count: 30
      }
      num_workers: 16
      batch_size: 128
    }
  }

  seed_everything: 42
  trainer: {
    gpus: 1
    auto_select_gpus: false

    max_epochs: 100

    checkpoint_callback: false
    deterministic: true
    profiler: simple
  }
  logger_name: cpc_v2_model

  params: {
    data_module_class: dltranz.data_load.data_module.cpc_v2_data_module.CpcV2DataModuleTrain
    pl_module_class: dltranz.lightning_modules.cpc_v2_module.CpcV2Module

    encoder_type: rnn
    rnn: {
      type: gru
      hidden_size: 512
      bidir: false
      trainable_starter: static
    }

    rnn_agg: {
      type: gru
      hidden_size: 128
      bidir: false
      trainable_starter: static
    }


    trx_encoder: {
      norm_embeddings: false
      embeddings_noise: 0.003
      embeddings: {
        mcc_code: {in: 200, out: 128}
        tr_type: {in: 100, out: 127}
      }
      numeric_values: {
        amount: identity
      }
    }
    cpc: {
      n_forward_steps: 6
      n_negatives: 20
    }

    lr_scheduler: {
      step_size: 5
      step_gamma: 0.5
    }

    train: {
      lr: 0.002
      weight_decay: 0.0
    }
    
  }

  model_path: "models/cpc_v2_model.p"

  include "dataset_inference_file.hocon"

  output: {
    path: data/cpc_v2_embeddings,
    format: pickle,
  }
}