{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ddef53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7431993",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a798aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../../data’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239M  100  239M    0     0  49.4M      0  0:00:04  0:00:04 --:--:-- 54.9M02k      0  0:05:49 --:--:--  0:05:49  701k\n",
      "Archive:  age-prediction-nti-sbebank-2019.zip\n",
      "  inflating: ../../data/test.csv     \n",
      "  inflating: ../../data/small_group_description.csv  \n",
      "  inflating: ../../data/train_target.csv  \n",
      "  inflating: ../../data/transactions_train.csv  \n",
      "  inflating: ../../data/transactions_test.csv  \n"
     ]
    }
   ],
   "source": [
    "! mkdir ../../data\n",
    "! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d ../../data\n",
    "! mv age-prediction-nti-sbebank-2019.zip ../../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"PysparkDataPreprocessor\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ade4b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "source_data = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5429dc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "|    33172|         8|         11|    13.887|\n",
      "|    33172|         9|         11|    15.983|\n",
      "|    33172|        10|         11|    21.341|\n",
      "|    33172|        11|         11|    17.941|\n",
      "|    33172|        12|         11|    17.726|\n",
      "|    33172|        13|         18|    47.397|\n",
      "|    33172|        13|          1|   220.009|\n",
      "|    33172|        13|         11|     9.067|\n",
      "|    33172|        16|          3|    18.319|\n",
      "|    33172|        16|          1|     9.846|\n",
      "|    33172|        16|         11|    19.666|\n",
      "|    33172|        17|         82|     2.544|\n",
      "|    33172|        19|          3|    16.388|\n",
      "|    33172|        19|         32|    45.795|\n",
      "|    33172|        19|          1|     4.184|\n",
      "|    33172|        19|         11|    15.479|\n",
      "|    33172|        20|          1|     35.17|\n",
      "|    33172|        23|         11|    18.847|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = source_data.alias('df')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fa3239da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__and__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '_asc_doc',\n",
       " '_asc_nulls_first_doc',\n",
       " '_asc_nulls_last_doc',\n",
       " '_bitwiseAND_doc',\n",
       " '_bitwiseOR_doc',\n",
       " '_bitwiseXOR_doc',\n",
       " '_contains_doc',\n",
       " '_desc_doc',\n",
       " '_desc_nulls_first_doc',\n",
       " '_desc_nulls_last_doc',\n",
       " '_endswith_doc',\n",
       " '_eqNullSafe_doc',\n",
       " '_isNotNull_doc',\n",
       " '_isNull_doc',\n",
       " '_jc',\n",
       " '_like_doc',\n",
       " '_rlike_doc',\n",
       " '_startswith_doc',\n",
       " 'alias',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'astype',\n",
       " 'between',\n",
       " 'bitwiseAND',\n",
       " 'bitwiseOR',\n",
       " 'bitwiseXOR',\n",
       " 'cast',\n",
       " 'contains',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'dropFields',\n",
       " 'endswith',\n",
       " 'eqNullSafe',\n",
       " 'getField',\n",
       " 'getItem',\n",
       " 'isNotNull',\n",
       " 'isNull',\n",
       " 'isin',\n",
       " 'like',\n",
       " 'name',\n",
       " 'otherwise',\n",
       " 'over',\n",
       " 'rlike',\n",
       " 'startswith',\n",
       " 'substr',\n",
       " 'when',\n",
       " 'withField']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mapping_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "525f36b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|      null|          4|    71.463|\n",
      "|    33172|      null|         35|    45.017|\n",
      "|    33172|      null|         11|    13.887|\n",
      "|    33172|      null|         11|    15.983|\n",
      "|    33172|        13|         11|    21.341|\n",
      "|    33172|        50|         11|    17.941|\n",
      "|    33172|      null|         11|    17.726|\n",
      "|    33172|      null|         18|    47.397|\n",
      "|    33172|      null|          1|   220.009|\n",
      "|    33172|      null|         11|     9.067|\n",
      "|    33172|      null|          3|    18.319|\n",
      "|    33172|      null|          1|     9.846|\n",
      "|    33172|      null|         11|    19.666|\n",
      "|    33172|      null|         82|     2.544|\n",
      "|    33172|      null|          3|    16.388|\n",
      "|    33172|      null|         32|    45.795|\n",
      "|    33172|      null|          1|     4.184|\n",
      "|    33172|      null|         11|    15.479|\n",
      "|    33172|      null|          1|     35.17|\n",
      "|    33172|      null|         11|    18.847|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "col = 'small_group'\n",
    "\n",
    "mapping = {100 + i: i + 1 for i, row in enumerate(df.select(col).distinct().collect())}\n",
    "mapping_expr = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "df = df.withColumn('trans_date', mapping_expr[F.col('trans_date')])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ff99b1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c917d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'148'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "\n",
    "ttt = chain(*mapping.items())\n",
    "next(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "81a104d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = chain((5,6,7), (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import chain\n",
    "\n",
    "from .base import DataPreprocessor\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PysparkDataPreprocessor(DataPreprocessor):\n",
    "    \"\"\"Data preprocessor based on pandas.DataFrame\n",
    "    During preprocessing it\n",
    "        * transform `cols_event_time` column with date and time\n",
    "        * encodes category columns `cols_category` into ints;\n",
    "        * apply logarithm transformation to `cols_log_norm' columns;\n",
    "        * groups flat data by `col_id`;\n",
    "        * arranges data into list of dicts with features\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_id : str\n",
    "        name of column with ids\n",
    "    cols_event_time : str,\n",
    "        name of column with time and date\n",
    "    cols_category : list[str],s\n",
    "        list of category columns\n",
    "    cols_log_norm : list[str],\n",
    "        list of columns to be logarithmed\n",
    "    cols_identity : list[str],\n",
    "        list of columns to be passed as is without any transformation\n",
    "    cols_target: List[str],\n",
    "        list of columns with target\n",
    "    time_transformation: str. Default: 'default'.\n",
    "        type of transformation to be applied to time column\n",
    "    print_dataset_info : bool. Default: False.\n",
    "        If True, print dataset stats during preprocessor fitting and data transformation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 col_id: str,\n",
    "                 cols_event_time: str,\n",
    "                 cols_category: List[str],\n",
    "                 cols_log_norm: List[str],\n",
    "                 cols_identity: List[str],\n",
    "                 cols_target: List[str] = [],\n",
    "                 time_transformation: str = 'default'):\n",
    "\n",
    "        super().__init__(col_id, cols_event_time, cols_category, cols_log_norm, cols_identity, cols_target)\n",
    "        self.time_transformation = time_transformation\n",
    "        self.time_min = None\n",
    "        \n",
    "        \n",
    "    def fit(self, df, **params):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dt : pandas.DataFrame with flat data\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted preprocessor.\n",
    "        \"\"\"\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            mapping = {row[col]: i + 1 for i, row in enumerate(df.select(col).distinct().collect())}\n",
    "            self.cols_category_mapping[col] = create_map([F.lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df = df.withColumn('sign', F.when(F.col(col) >= 0, 1).otherwise(-1))\n",
    "            self.cols_log_norm_maxes[col] = source_data.select((F.log1p(F.abs(F.col(col))) *\n",
    "                                                                F.col('sign')).alias('log1p_signed'))\\\n",
    "                                            .agg({\"log1p_signed\": \"max\"}).collect()[0]['max(log1p_signed)']\n",
    "\n",
    "        if self.time_transformation == 'hours_from_min':\n",
    "            self.time_min = df.select((F.col(self.cols_event_time))\\\n",
    "                                      .cast(dataType=T.TimestampType()).alias('dt'))\\\n",
    "                                      .agg({'dt': 'min'}).collect()[0]['min(dt)']\n",
    "            self.time_min = (self.time_min - datetime.datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, df, copy=True):\n",
    "        \"\"\"Perform preprocessing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame with flat data\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "        Returns\n",
    "        -------\n",
    "        features : List of dicts grouped by col_id.\n",
    "        \"\"\"\n",
    "        self.check_is_fitted()\n",
    "        df_data = df.alias('df_data') if copy else df\n",
    "\n",
    "        # event_time mapping\n",
    "        if self.time_transformation == 'none':\n",
    "            pass\n",
    "        elif self.time_transformation == 'default':\n",
    "            df_data = self._td_default(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'float':\n",
    "            df_data = self._td_float(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'gender':\n",
    "            df_data = self._td_gender(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'hours_from_min':\n",
    "            df_data = self._td_hours(df_data, self.cols_event_time)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown type of data transformation: \"{self.time_transformation}\"')\n",
    "            \n",
    "        for col in self.cols_category:\n",
    "            if col not in self.cols_category_mapping:\n",
    "                raise KeyError(f\"column {col} isn't in fitted category columns\")\n",
    "            mapping_expr = F.create_map([F.lit(x) for x in chain(*self.cols_category_mapping[col].items())])  \n",
    "            df_data = df_data.withColumn(col, mapping_expr[F.col(col)])\n",
    "            val_for_null = max(self.cols_category_mapping[col].values())\n",
    "            df_data = df_data.fillna(value=val_for_null, subset=[col])\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df_data = df_data.withColumn('_sign', F.when(F.col(col) >= 0, 1).otherwise(-1))\n",
    "            df_data = df_data.withColumn(col, F.log1p(F.abs(F.col(col))) * F.col('_sign')\n",
    "                                         / self.cols_log_norm_maxes[col])\n",
    "            df_data = df_data.drop('_sign')\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    def TRANSFORM(self, df, copy=True):\n",
    "        \"\"\"Perform preprocessing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame with flat data\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "        Returns\n",
    "        -------\n",
    "        features : List of dicts grouped by col_id.\n",
    "        \"\"\"\n",
    "        self.check_is_fitted()\n",
    "        df_data = df.copy() if copy else df\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            logger.info(f'Found {df_data[self.col_id].nunique()} unique ids')\n",
    "\n",
    "        # event_time mapping\n",
    "        if self.time_transformation == 'none':\n",
    "            pass\n",
    "        elif self.time_transformation == 'default':\n",
    "            df_data = self._td_default(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'float':\n",
    "            df_data = self._td_float(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'gender':\n",
    "            df_data = self._td_gender(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'hours_from_min':\n",
    "            df_data = self._td_hours(df_data, self.cols_event_time)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown type of data transformation: \"{self.time_transformation}\"')\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            if col not in self.cols_category_mapping:\n",
    "                raise KeyError(f\"column {col} isn't in fitted category columns\")\n",
    "            pd_col = df_data[col].astype(str)\n",
    "            df_data[col] = pd_col.map(self.cols_category_mapping[col]) \\\n",
    "                .fillna(max(self.cols_category_mapping[col].values()))\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(df_data[col], col)}')\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df_data[col] = np.log1p(abs(df_data[col])) * np.sign(df_data[col])\n",
    "            df_data[col] /= self.cols_log_norm_maxes[col]\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(df_data[col], col)}')\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            df = df_data.groupby(self.col_id)['event_time'].count()\n",
    "            logger.info(f'Trx count per clients:\\nlen(trx_list) | client_count\\n{pd_hist(df, \"trx_count\")}')\n",
    "\n",
    "        # column filter\n",
    "        columns_for_filter = reduce(iadd, [\n",
    "            self.cols_category,\n",
    "            self.cols_log_norm,\n",
    "            self.cols_identity,\n",
    "            ['event_time', self.col_id],\n",
    "            self.cols_target,\n",
    "        ], [])\n",
    "        used_columns = [col for col in df_data.columns if col in columns_for_filter]\n",
    "\n",
    "        logger.info('Feature collection in progress ...')\n",
    "        features = df_data[used_columns] \\\n",
    "            .assign(et_index=lambda x: x['event_time']) \\\n",
    "            .set_index([self.col_id, 'et_index']).sort_index() \\\n",
    "            .groupby(self.col_id).apply(lambda x: {k: v[0] if k in self.cols_target else np.array(v)\n",
    "                                                   for k, v in x.to_dict(orient='list').items()}) \\\n",
    "            .rename('feature_arrays').reset_index().to_dict(orient='records')\n",
    "\n",
    "        def squeeze(rec):\n",
    "            return {self.col_id: rec[self.col_id], **rec['feature_arrays']}\n",
    "        features = [squeeze(r) for r in features]\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            feature_names = list(features[0].keys())\n",
    "            logger.info(f'Feature names: {feature_names}')\n",
    "\n",
    "        logger.info(f'Prepared features for {len(features)} clients')\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _td_default(df, cols_event_time):\n",
    "        w = Window().orderBy(cols_event_time)\n",
    "        tmp_df = df.select(cols_event_time).distinct()\n",
    "        tmp_df = tmp_df.withColumn('event_time', F.row_number().over(w) - 1)\n",
    "        df = df.join(tmp_df, on=cols_event_time)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _td_float(df, col_event_time):\n",
    "        logger.info('To-float time transformation begins...')\n",
    "        df = df.withColumn('event_time', F.col(col_event_time).astype('float'))\n",
    "        logger.info('To-float time transformation ends')\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _td_gender(df, col_event_time):\n",
    "        \"\"\"Gender-dataset-like transformation\n",
    "        'd hh:mm:ss' -> float where integer part is day number and fractional part is seconds from day begin\n",
    "        '1 00:00:00' -> 1.0\n",
    "        '1 12:00:00' -> 1.5\n",
    "        '1 01:00:00' -> 1 + 1 / 24\n",
    "        '2 23:59:59' -> 1.99\n",
    "        '432 12:00:00' -> 432.5   '000432 12:00:00'\n",
    "        :param df:\n",
    "        :param col_event_time:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logger.info('Gender-dataset-like time transformation begins...')\n",
    "        df = df.withColumn('_et_day', F.substring(F.lpad(F.col(col_event_time), 15, '0'), 1, 6).cast('float'))\n",
    "\n",
    "        df = df.withColumn('_et_time', F.substring(F.lpad(F.col(col_event_time), 15, '0'), 8, 8))\n",
    "        df = df.withColumn('_et_time', F.regexp_replace('_et_time', r'\\:60$', ':59'))\n",
    "        df = df.withColumn('_et_time', F.unix_timestamp('_et_time', 'HH:mm:ss') / (24 * 60 * 60))\n",
    "\n",
    "        df = df.withColumn('event_time', F.col('_et_day') + F.col('_et_time'))\n",
    "        df = df.drop('_et_day', '_et_time')\n",
    "        logger.info('Gender-dataset-like time transformation ends')\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def _td_hours(self, df, col_event_time):\n",
    "        logger.info('To hours time transformation begins...')\n",
    "        df = df.withColumn('_dt', (F.col(col_event_time)).cast(dataType=T.TimestampType()))\n",
    "        df = df.withColumn('event_time', ((F.col('_dt')).cast('float') - self.time_min) / 3600)\n",
    "        df = df.drop('_dt')\n",
    "        logger.info('To hours time transformation ends')\n",
    "        return df\n",
    "\n",
    "   \n",
    "    def _reset(self):\n",
    "        \"\"\"Reset internal data-dependent state of the preprocessor, if necessary.\n",
    "        __init__ parameters are not touched.\n",
    "        \"\"\"\n",
    "        self.time_min = None\n",
    "        super()._reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7e06bd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   client_id  trans_date  small_group  amount_rur\n",
       "0      33172           6            4      71.463\n",
       "1      33172           6           35      45.017"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "source_data_pd = pd.read_csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac13ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9e5300ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>71.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>45.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33172</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33172</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>15.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33172</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>21.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450572</th>\n",
       "      <td>43300</td>\n",
       "      <td>727</td>\n",
       "      <td>9</td>\n",
       "      <td>7.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450573</th>\n",
       "      <td>43300</td>\n",
       "      <td>727</td>\n",
       "      <td>4</td>\n",
       "      <td>3.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450574</th>\n",
       "      <td>43300</td>\n",
       "      <td>727</td>\n",
       "      <td>1</td>\n",
       "      <td>6.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450575</th>\n",
       "      <td>43300</td>\n",
       "      <td>727</td>\n",
       "      <td>2</td>\n",
       "      <td>24.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450576</th>\n",
       "      <td>43300</td>\n",
       "      <td>729</td>\n",
       "      <td>3</td>\n",
       "      <td>19.408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26450577 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          client_id  trans_date  small_group  amount_rur\n",
       "0             33172           6            5      71.463\n",
       "1             33172           6           21      45.017\n",
       "2             33172           8            2      13.887\n",
       "3             33172           9            2      15.983\n",
       "4             33172          10            2      21.341\n",
       "...             ...         ...          ...         ...\n",
       "26450572      43300         727            9       7.602\n",
       "26450573      43300         727            4       3.709\n",
       "26450574      43300         727            1       6.448\n",
       "26450575      43300         727            2      24.669\n",
       "26450576      43300         729            3      19.408\n",
       "\n",
       "[26450577 rows x 4 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = source_data_pd.copy()\n",
    "col = 'small_group'\n",
    "\n",
    "\n",
    "pd_col = df_data[col].astype(str)\n",
    "mapping = {k: i + 1 for i, k in enumerate(pd_col.value_counts().index)}\n",
    "\n",
    "\n",
    "df_data[col] = pd_col.map(mapping)  #.fillna(max(self.cols_category_mapping[col].values()))\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aae52323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dda7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df00e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf45735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5fdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca299eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8615b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_preprocessing import PandasDataPreprocessor\n",
    "\n",
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    cols_event_time='trans_date',\n",
    "    time_transformation='float',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[\"amount_rur\"],\n",
    "    cols_identity=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca72f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 3.2 s, total: 53.6 s\n",
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = preprocessor.fit_transform(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e7d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e2418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9080a6f",
   "metadata": {},
   "source": [
    "## Embedding training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56493c0b",
   "metadata": {},
   "source": [
    "Model training in our framework organised via pytorch-lightning (pl) framework.\n",
    "The key parts of neural networks training in pl are: \n",
    "\n",
    "    * model (pl.LightningModule)\n",
    "    * data_module (pl.LightningDataModule)\n",
    "    * pl.trainer (pl.trainer)\n",
    "    \n",
    "For futher details check https://www.pytorchlightning.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6ee58",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988c508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder import SequenceEncoder\n",
    "from dltranz.models import Head\n",
    "from dltranz.lightning_modules.emb_module import EmbModule\n",
    "\n",
    "seq_encoder = SequenceEncoder(\n",
    "    category_features=preprocessor.get_category_sizes(),\n",
    "    numeric_features=[\"amount_rur\"],\n",
    "    trx_embedding_noize=0.003\n",
    ")\n",
    "\n",
    "head = Head(input_size=seq_encoder.embedding_size, use_norm_encoder=True)\n",
    "\n",
    "model = EmbModule(seq_encoder=seq_encoder, head=head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87339c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87997ac0",
   "metadata": {},
   "source": [
    "### Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624065bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule\n",
    "\n",
    "dm = EmbeddingTrainDataModule(\n",
    "    dataset=train,\n",
    "    pl_module=model,\n",
    "    min_seq_len=25,\n",
    "    seq_split_strategy='SampleSlices',\n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    "    split_count=5,\n",
    "    split_cnt_min=25,\n",
    "    split_cnt_max=200,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a09be",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdbb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "# logging.getLogger(\"lightning\").addHandler(logging.NullHandler())\n",
    "# logging.getLogger(\"lightning\").propagate = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "#     progress_bar_refresh_rate=0,\n",
    "    max_epochs=150,\n",
    "    gpus=1 if torch.cuda.is_available() else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88078a3",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40877df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c32741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24000it [00:01, 21719.33it/s]\n",
      "6000it [00:00, 31278.41it/s]                                                                                                            \n",
      "                                                                                                                                        \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((24000, 512), (6000, 512))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding inference\n",
    "\n",
    "from dltranz.inference import get_embeddings\n",
    "\n",
    "train_embeds = get_embeddings(\n",
    "    data=train,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "test_embeds = get_embeddings(\n",
    "    data=test,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18245f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 514) (6000, 514)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_504</th>\n",
       "      <th>embed_505</th>\n",
       "      <th>embed_506</th>\n",
       "      <th>embed_507</th>\n",
       "      <th>embed_508</th>\n",
       "      <th>embed_509</th>\n",
       "      <th>embed_510</th>\n",
       "      <th>embed_511</th>\n",
       "      <th>client_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344823</td>\n",
       "      <td>0.340152</td>\n",
       "      <td>0.231864</td>\n",
       "      <td>-0.789117</td>\n",
       "      <td>-0.013289</td>\n",
       "      <td>-0.056129</td>\n",
       "      <td>-0.988241</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>-0.064898</td>\n",
       "      <td>-0.029179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342959</td>\n",
       "      <td>0.040367</td>\n",
       "      <td>0.253053</td>\n",
       "      <td>0.712581</td>\n",
       "      <td>-0.148498</td>\n",
       "      <td>0.016645</td>\n",
       "      <td>-0.124844</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>36253</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276711</td>\n",
       "      <td>0.492666</td>\n",
       "      <td>0.781279</td>\n",
       "      <td>-0.824952</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>-0.014695</td>\n",
       "      <td>-0.891105</td>\n",
       "      <td>-0.047485</td>\n",
       "      <td>0.063514</td>\n",
       "      <td>0.170822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372329</td>\n",
       "      <td>0.049816</td>\n",
       "      <td>0.346733</td>\n",
       "      <td>0.465750</td>\n",
       "      <td>-0.080580</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>-0.040877</td>\n",
       "      <td>-0.028394</td>\n",
       "      <td>396</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0  0.344823  0.340152  0.231864 -0.789117 -0.013289 -0.056129 -0.988241   \n",
       "1  0.276711  0.492666  0.781279 -0.824952  0.020340 -0.014695 -0.891105   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_504  embed_505  embed_506  \\\n",
       "0 -0.010464 -0.064898 -0.029179  ...   0.342959   0.040367   0.253053   \n",
       "1 -0.047485  0.063514  0.170822  ...   0.372329   0.049816   0.346733   \n",
       "\n",
       "   embed_507  embed_508  embed_509  embed_510  embed_511  client_id  target  \n",
       "0   0.712581  -0.148498   0.016645  -0.124844  -0.078120      36253       1  \n",
       "1   0.465750  -0.080580   0.011563  -0.040877  -0.028394        396       2  \n",
       "\n",
       "[2 rows x 514 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join target and embeddings\n",
    "\n",
    "df_target = pd.read_csv(os.path.join(data_path, 'train_target.csv'))\n",
    "df_target = df_target.set_index('client_id')\n",
    "df_target.rename(columns={\"bins\": \"target\"}, inplace=True)\n",
    "\n",
    "train_df = pd.DataFrame(data=train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n",
    "train_df['client_id'] = [x['client_id'] for x in train]\n",
    "train_df = train_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "test_df = pd.DataFrame(data=test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n",
    "test_df['client_id'] = [x['client_id'] for x in test]\n",
    "test_df = test_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa0c0",
   "metadata": {},
   "source": [
    "Obtained embeddings can be used as features for model training\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e3de46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embed_columns = [x for x in train_df.columns if x.startswith('embed')]\n",
    "x_train, y_train = train_df[embed_columns], train_df['target']\n",
    "x_test, y_test = test_df[embed_columns], test_df['target']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
